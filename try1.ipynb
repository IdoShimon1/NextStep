{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ylopV4WQZjN"
      },
      "source": [
        "imports"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWFegCyDQbEk"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EDS1FEKoQbhO",
        "outputId": "32ac3cc8-fb1b-4076-9fec-3f4ee321db49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DataFrame shape: (9544, 35)\n",
            "Columns: ['address', 'career_objective', 'skills', 'educational_institution_name', 'degree_names', 'passing_years', 'educational_results', 'result_types', 'major_field_of_studies', 'professional_company_names', 'company_urls', 'start_dates', 'end_dates', 'related_skils_in_job', 'positions', 'locations', 'responsibilities', 'extra_curricular_activity_types', 'extra_curricular_organization_names', 'extra_curricular_organization_links', 'role_positions', 'languages', 'proficiency_levels', 'certification_providers', 'certification_skills', 'online_links', 'issue_dates', 'expiry_dates', 'job_position_name', 'educationaL_requirements', 'experiencere_requirement', 'age_requirement', 'responsibilities.1', 'skills_required', 'matched_score']\n",
            "Number of distinct first-position labels: 223\n",
            "\n",
            "=== No saved embeddings found, computing with BERT... ===\n",
            "Embedded 100 rows...\n",
            "Embedded 200 rows...\n",
            "Embedded 300 rows...\n",
            "Embedded 400 rows...\n",
            "Embedded 500 rows...\n",
            "Embedded 600 rows...\n",
            "Embedded 700 rows...\n",
            "Embedded 800 rows...\n",
            "Embedded 900 rows...\n",
            "Embedded 1000 rows...\n",
            "Embedded 1100 rows...\n",
            "Embedded 1200 rows...\n",
            "Embedded 1300 rows...\n",
            "Embedded 1400 rows...\n",
            "Embedded 1500 rows...\n",
            "Embedded 1600 rows...\n",
            "Embedded 1700 rows...\n",
            "Embedded 1800 rows...\n",
            "Embedded 1900 rows...\n",
            "Embedded 2000 rows...\n",
            "Embedded 2100 rows...\n",
            "Embedded 2200 rows...\n",
            "Embedded 2300 rows...\n",
            "Embedded 2400 rows...\n",
            "Embedded 2500 rows...\n",
            "Embedded 2600 rows...\n",
            "Embedded 2700 rows...\n",
            "Embedded 2800 rows...\n",
            "Embedded 2900 rows...\n",
            "Embedded 3000 rows...\n",
            "Embedded 3100 rows...\n",
            "Embedded 3200 rows...\n",
            "Embedded 3300 rows...\n",
            "Embedded 3400 rows...\n",
            "Embedded 3500 rows...\n",
            "Embedded 3600 rows...\n",
            "Embedded 3700 rows...\n",
            "Embedded 3800 rows...\n",
            "Embedded 3900 rows...\n",
            "Embedded 4000 rows...\n",
            "Embedded 4100 rows...\n",
            "Embedded 4200 rows...\n",
            "Embedded 4300 rows...\n",
            "Embedded 4400 rows...\n",
            "Embedded 4500 rows...\n",
            "Embedded 4600 rows...\n",
            "Embedded 4700 rows...\n",
            "Embedded 4800 rows...\n",
            "Embedded 4900 rows...\n",
            "Embedded 5000 rows...\n",
            "Embedded 5100 rows...\n",
            "Embedded 5200 rows...\n",
            "Embedded 5300 rows...\n",
            "Embedded 5400 rows...\n",
            "Embedded 5500 rows...\n",
            "Embedded 5600 rows...\n",
            "Embedded 5700 rows...\n",
            "Embedded 5800 rows...\n",
            "Embedded 5900 rows...\n",
            "Embedded 6000 rows...\n",
            "Embedded 6100 rows...\n",
            "Embedded 6200 rows...\n",
            "Embedded 6300 rows...\n",
            "Embedded 6400 rows...\n",
            "Embedded 6500 rows...\n",
            "Embedded 6600 rows...\n",
            "Embedded 6700 rows...\n",
            "Embedded 6800 rows...\n",
            "Embedded 6900 rows...\n",
            "Embedded 7000 rows...\n",
            "Embedded 7100 rows...\n",
            "Embedded 7200 rows...\n",
            "Embedded 7300 rows...\n",
            "Embedded 7400 rows...\n",
            "Embedded 7500 rows...\n",
            "Embedded 7600 rows...\n",
            "Embedded 7700 rows...\n",
            "Embedded 7800 rows...\n",
            "Embedded 7900 rows...\n",
            "Embedded 8000 rows...\n",
            "Embedded 8100 rows...\n",
            "Embedded 8200 rows...\n",
            "Embedded 8300 rows...\n",
            "Embedded 8400 rows...\n",
            "Embedded 8500 rows...\n",
            "Embedded 8600 rows...\n",
            "Embedded 8700 rows...\n",
            "Embedded 8800 rows...\n",
            "Embedded 8900 rows...\n",
            "Embedded 9000 rows...\n",
            "Embedded 9100 rows...\n",
            "Embedded 9200 rows...\n",
            "Embedded 9300 rows...\n",
            "Embedded 9400 rows...\n",
            "Embedded 9500 rows...\n",
            "Final embedding shape: (9544, 768)\n",
            "Embeddings saved as 'X_embeddings.npy' and 'y_labels.npy'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">196,864</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">32,896</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">223</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">28,767</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m196,864\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m32,896\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m223\u001b[0m)                 │          \u001b[38;5;34m28,767\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">258,527</span> (1009.87 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m258,527\u001b[0m (1009.87 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">258,527</span> (1009.87 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m258,527\u001b[0m (1009.87 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.1148 - loss: 4.6960 - val_accuracy: 0.3981 - val_loss: 2.4548\n",
            "Epoch 2/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.4081 - loss: 2.2378 - val_accuracy: 0.8413 - val_loss: 0.7778\n",
            "Epoch 3/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.7480 - loss: 0.9099 - val_accuracy: 0.9785 - val_loss: 0.2200\n",
            "Epoch 4/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.8848 - loss: 0.4303 - val_accuracy: 0.9932 - val_loss: 0.0695\n",
            "Epoch 5/5\n",
            "\u001b[1m239/239\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step - accuracy: 0.9384 - loss: 0.2451 - val_accuracy: 0.9979 - val_loss: 0.0324\n",
            "\n",
            "Test Accuracy: 0.9979\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\n",
            "New sample text: Python,Machine Learning,Deep Learning Data Science M.Sc (Data Science) ML Engineer Project Lead\n",
            "Predicted FIRST position label: Computer Vision Scientist\n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q transformers torch scikit-learn pandas tensorflow \n",
        "\n",
        "import os\n",
        "import re\n",
        "import ast\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "##########################\n",
        "# 1) LOAD CSV & CHECK\n",
        "##########################\n",
        "\n",
        "csv_path = 'resume_data.csv' \n",
        "df = pd.read_csv(csv_path)\n",
        "print(\"DataFrame shape:\", df.shape)\n",
        "print(\"Columns:\", df.columns.tolist())\n",
        "\n",
        "# Make sure these columns exist: 'skills', 'major_field_of_studies', 'degree_names', 'positions'.\n",
        "for req_col in [\"skills\", \"major_field_of_studies\", \"degree_names\", \"positions\"]:\n",
        "    if req_col not in df.columns:\n",
        "        raise ValueError(f\"Column '{req_col}' missing from CSV. Please check your data.\")\n",
        "\n",
        "df[\"skills\"]                 = df[\"skills\"].fillna(\"\")\n",
        "df[\"major_field_of_studies\"] = df[\"major_field_of_studies\"].fillna(\"\")\n",
        "df[\"degree_names\"]           = df[\"degree_names\"].fillna(\"\")\n",
        "df[\"positions\"]              = df[\"positions\"].fillna(\"[]\")\n",
        "\n",
        "##########################\n",
        "# 2) PARSE LABEL = FIRST POSITION\n",
        "##########################\n",
        "\n",
        "def parse_positions_and_label(pos_str):\n",
        "    \"\"\"\n",
        "    Parse 'positions' as a Python list.\n",
        "    Label = the FIRST item, leftover = everything else.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        pos_list = ast.literal_eval(pos_str)\n",
        "        if isinstance(pos_list, list) and len(pos_list) > 0:\n",
        "            label = pos_list[0].strip() if pos_list[0] else \"\"\n",
        "            leftover = pos_list[1:]  # everything after first\n",
        "            return label, leftover\n",
        "        else:\n",
        "            return \"\", []\n",
        "    except:\n",
        "        return pos_str, []  # fallback if parse fails\n",
        "\n",
        "df_labels = []\n",
        "df_positions_for_embed = []\n",
        "for p in df[\"positions\"]:\n",
        "    label, leftover = parse_positions_and_label(p)\n",
        "    df_labels.append(label)\n",
        "    df_positions_for_embed.append(leftover)\n",
        "\n",
        "df[\"first_position_label\"]   = df_labels\n",
        "df[\"positions_for_embedding\"] = df_positions_for_embed\n",
        "\n",
        "# Encode label\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df[\"first_position_label\"].values)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(\"Number of distinct first-position labels:\", num_classes)\n",
        "\n",
        "##########################\n",
        "# 3) BUILD TEXT FOR BERT\n",
        "##########################\n",
        "\n",
        "def build_text_for_embedding(row):\n",
        "    \"\"\"\n",
        "    We ONLY use columns: 'skills', 'major_field_of_studies',\n",
        "    'degree_names', and leftover positions (skipping the first position which is our label).\n",
        "    \"\"\"\n",
        "    text_parts = []\n",
        "\n",
        "    # A) Skills might be \"['Skill1','Skill2']\"\n",
        "    s = row[\"skills\"]\n",
        "    s_clean = re.sub(r\"[\\[\\]']\", \"\", s)  # remove brackets, quotes\n",
        "    text_parts.append(s_clean.strip())\n",
        "\n",
        "    # B) Major field\n",
        "    m = row[\"major_field_of_studies\"]\n",
        "    text_parts.append(m.strip())\n",
        "\n",
        "    # C) Degree names (again may have brackets)\n",
        "    d = row[\"degree_names\"]\n",
        "    d_clean = re.sub(r\"[\\[\\]']\", \"\", d)\n",
        "    text_parts.append(d_clean.strip())\n",
        "\n",
        "    # D) leftover positions => row[\"positions_for_embedding\"] is a list\n",
        "    leftover_positions = row[\"positions_for_embedding\"]\n",
        "    if isinstance(leftover_positions, list):\n",
        "        # Convert every element to a string, ignoring None\n",
        "        leftover_positions_str = [str(x) for x in leftover_positions if x is not None]\n",
        "        leftover_str = \" \".join(leftover_positions_str)\n",
        "        text_parts.append(leftover_str.strip())\n",
        "\n",
        "    # Join all\n",
        "    return \" \".join(x for x in text_parts if x).strip()\n",
        "\n",
        "##########################\n",
        "# 4) BERT EMBEDDING FN\n",
        "##########################\n",
        "\n",
        "@torch.no_grad()\n",
        "def get_bert_embedding(text, tokenizer, bert_model, device):\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=128,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "    for k, v in inputs.items():\n",
        "        inputs[k] = v.to(device)\n",
        "\n",
        "    outputs = bert_model(**inputs)\n",
        "    last_hidden_state = outputs.last_hidden_state\n",
        "    emb = last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
        "    return emb\n",
        "\n",
        "##########################\n",
        "# 5) BUILD or LOAD X\n",
        "##########################\n",
        "\n",
        "X_filename = \"X_embeddings.npy\"\n",
        "y_filename = \"y_labels.npy\"\n",
        "\n",
        "if os.path.exists(X_filename) and os.path.exists(y_filename):\n",
        "    print(\"\\n=== Embeddings found. Loading them. ===\")\n",
        "    X = np.load(X_filename)\n",
        "    y_loaded = np.load(y_filename)\n",
        "    if len(y_loaded) == len(y):\n",
        "        y = y_loaded\n",
        "    else:\n",
        "        print(\"WARNING: loaded y doesn't match current data. Using newly computed y.\")\n",
        "else:\n",
        "    print(\"\\n=== No saved embeddings found, computing with BERT... ===\")\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    bert_model = AutoModel.from_pretrained(model_name)\n",
        "    bert_model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    bert_model.to(device)\n",
        "\n",
        "    all_texts = df.apply(build_text_for_embedding, axis=1)\n",
        "    X_list = []\n",
        "    for i, txt in enumerate(all_texts):\n",
        "        emb = get_bert_embedding(txt, tokenizer, bert_model, device)\n",
        "        X_list.append(emb)\n",
        "        if (i+1) % 100 == 0:\n",
        "            print(f\"Embedded {i+1} rows...\")\n",
        "    X = np.vstack(X_list)\n",
        "    print(\"Final embedding shape:\", X.shape)\n",
        "\n",
        "    # Save\n",
        "    np.save(X_filename, X)\n",
        "    np.save(y_filename, y)\n",
        "    print(f\"Embeddings saved as '{X_filename}' and '{y_filename}'.\")\n",
        "\n",
        "##########################\n",
        "# 6) TRAIN MODEL\n",
        "##########################\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "y_train_oh = to_categorical(y_train, num_classes)\n",
        "y_test_oh  = to_categorical(y_test, num_classes)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256, activation='relu', input_shape=(X.shape[1],)))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train_oh,\n",
        "    validation_data=(X_test, y_test_oh),\n",
        "    epochs=5,  # adjust as needed\n",
        "    batch_size=32,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test_oh, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "##########################\n",
        "# 7) EXAMPLE INFERENCE\n",
        "##########################\n",
        "\n",
        "new_row = {\n",
        "    \"skills\": \"['Python','Machine Learning','Deep Learning']\",\n",
        "    \"major_field_of_studies\": \"Data Science\",\n",
        "    \"degree_names\": \"['M.Sc (Data Science)']\",\n",
        "    \"positions\": \"['Software Developer','ML Engineer','Project Lead']\"\n",
        "}\n",
        "\n",
        "def parse_positions_for_new_sample(pos_str):\n",
        "    try:\n",
        "        arr = ast.literal_eval(pos_str)\n",
        "        if isinstance(arr, list) and len(arr) > 0:\n",
        "            # label = arr[0], leftover = arr[1:]\n",
        "            leftover = arr[1:]\n",
        "            return leftover\n",
        "        else:\n",
        "            return []\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "leftover_positions = parse_positions_for_new_sample(new_row[\"positions\"])\n",
        "\n",
        "def build_text_for_new_inference(row, leftover):\n",
        "    text_parts = []\n",
        "\n",
        "    # Skills\n",
        "    s = row[\"skills\"]\n",
        "    s_clean = re.sub(r\"[\\[\\]']\", \"\", s)\n",
        "    text_parts.append(s_clean.strip())\n",
        "\n",
        "    # major\n",
        "    text_parts.append(row[\"major_field_of_studies\"].strip())\n",
        "\n",
        "    # degree\n",
        "    d = row[\"degree_names\"]\n",
        "    d_clean = re.sub(r\"[\\[\\]']\", \"\", d)\n",
        "    text_parts.append(d_clean.strip())\n",
        "\n",
        "    # leftover positions\n",
        "    leftover_str_list = [str(x) for x in leftover if x is not None]\n",
        "    leftover_str = \" \".join(leftover_str_list)\n",
        "    text_parts.append(leftover_str.strip())\n",
        "\n",
        "    return \" \".join(x for x in text_parts if x).strip()\n",
        "\n",
        "inf_text = build_text_for_new_inference(new_row, leftover_positions)\n",
        "\n",
        "# If we loaded from .npy, we might need to re-load BERT for inference:\n",
        "if 'bert_model' not in globals():\n",
        "    print(\"Re-loading BERT for inference...\")\n",
        "    model_name = \"bert-base-uncased\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    bert_model = AutoModel.from_pretrained(model_name)\n",
        "    bert_model.eval()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    bert_model.to(device)\n",
        "\n",
        "inf_emb = get_bert_embedding(inf_text, tokenizer, bert_model, device).reshape(1, -1)\n",
        "pred_probs = model.predict(inf_emb)\n",
        "pred_idx = np.argmax(pred_probs, axis=1)[0]\n",
        "pred_label = label_encoder.inverse_transform([pred_idx])[0]\n",
        "\n",
        "print(\"\\nNew sample text:\", inf_text)\n",
        "print(\"Predicted FIRST position label:\", pred_label)\n",
        "print(\"\\nDone!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6yuWQcnJdhFG"
      },
      "outputs": [],
      "source": [
        "# Save the array X to a file named \"X_embeddings.npy\"\n",
        "np.save('X_embeddings.npy', X)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
